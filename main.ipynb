{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224,224)\n",
    "IMAGE_PATH = os.path.join(os.getcwd(), 'images')\n",
    "TOTAL_NUMBER_OF_IMAGES = len(os.listdir(IMAGE_PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BREED_DICT = { # 0 for cat, 1 for dog\n",
    "    'persian': 0,\n",
    "    'ragdoll': 0,\n",
    "    'havanese': 1,\n",
    "    'sphynx': 0,\n",
    "    'keeshond': 1,\n",
    "    'beagle': 1,\n",
    "    'leonberger': 1,\n",
    "    'english_cocker_spaniel': 1,\n",
    "    'german_shorthaired': 1,\n",
    "    'saint_bernard': 1,\n",
    "    'japanese_chin': 1,\n",
    "    'pug': 1,\n",
    "    'newfoundland': 1,\n",
    "    'yorkshire_terrier': 1,\n",
    "    'basset_hound': 1,\n",
    "    'birman': 0,\n",
    "    'great_pyrenees': 1,\n",
    "    'boxer': 1,\n",
    "    'staffordshire_bull_terrier': 1,\n",
    "    'wheaten_terrier': 1,\n",
    "    'egyptian_mau': 0,\n",
    "    'maine_coon': 0,\n",
    "    'american_pit_bull_terrier': 1,\n",
    "    'shiba_inu': 1,\n",
    "    'miniature_pinscher': 1,\n",
    "    'samoyed': 1,\n",
    "    'abyssinian': 0,\n",
    "    'pomeranian': 1,\n",
    "    'russian_blue': 0,\n",
    "    'bombay': 0,\n",
    "    'english_setter': 1,\n",
    "    'chihuahua': 1,\n",
    "    'british_shorthair': 0,\n",
    "    'siamese': 0,\n",
    "    'bengal': 0,\n",
    "    'american_bulldog': 1,\n",
    "    'scottish_terrier': 1\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "from torchvision.transforms import v2 as T\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_breed(image_path):\n",
    "    name = image_path.split('.')[0]\n",
    "    # Split on last _\n",
    "    name = '_'.join(name.rsplit('_', -1)[:-1]).lower()\n",
    "\n",
    "    return name\n",
    "\n",
    "def load_classes(dataset_path):\n",
    "    classes = set()\n",
    "\n",
    "    for img in os.listdir(dataset_path):\n",
    "        name = get_breed(img)\n",
    "        classes.add(name)\n",
    "\n",
    "    return classes\n",
    "\n",
    "classes = load_classes(IMAGE_PATH)\n",
    "print(f\"{len(classes)} classes found\")\n",
    "\n",
    "def imshow(path: torch.Tensor):\n",
    "    img = path.permute(1, 2, 0)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class CatAndDogsDataset(Dataset):\n",
    "    def __init__(self, image_path, img_size, transform=None):\n",
    "        self.image_path = image_path\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "        self.image_files = os.listdir(self.image_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_path, self.image_files[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = transforms.Resize(self.img_size)(img)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        one_hot = np.zeros(2)\n",
    "        label = BREED_DICT[get_breed(self.image_files[idx])]\n",
    "        one_hot[label] = 1\n",
    "\n",
    "        return img, one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CatAndDogsDataset(IMAGE_PATH, IMG_SIZE, transform=transforms.ToTensor())\n",
    "# Take only 128  images\n",
    "dataset.image_files = dataset.image_files[:128]\n",
    "\n",
    "# Get some data\n",
    "random_index = np.random.randint(0, len(dataset))\n",
    "data, one_hot = dataset[random_index]\n",
    "\n",
    "print(f\"ITS A {'dog' if one_hot[1] == 1 else 'cat'}\")\n",
    "\n",
    "imshow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "model = torchvision.models.resnet18(weights=\"DEFAULT\")\n",
    "\n",
    "num_classes = 2  # 2 classes: cat and dogs\n",
    "\n",
    "in_features = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(in_features, num_classes)\n",
    "\n",
    "loss_criter = nn.CrossEntropyLoss()\n",
    "\n",
    "# Use nADAM\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "val_size = int(0.2 * len(dataset))  # 20% for validation\n",
    "train_size = len(dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# 80% of the data is used for training, 20% for validation\n",
    "train_data = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_data = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_over_time = []\n",
    "correct = 0\n",
    "\n",
    "for epoch in range(2):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(train_data, 0)):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_criter(outputs, labels)\n",
    "\n",
    "        # Check if the model predicted the right class\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        labels = torch.argmax(labels, 1)  # convert from one-hot encoding to class labels\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        loss_over_time.append(loss.item())\n",
    "        if i % 2000 == 1999:\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 2000}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(f\"Accuracy: {correct / 256}\")\n",
    "print(running_loss)\n",
    "plt.plot(loss_over_time)\n",
    "print(\"Finished Training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
